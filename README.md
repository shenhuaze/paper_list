# Paper List
Paper list of NLP, RecSys, Chatbot, and CV.

[NLP](#NLP)

[RecSys](#RecSys)

[Chatbot](#Chatbot)

[CV](#CV)

## NLP

| 标题                                                         | 备注                                      |
| ------------------------------------------------------------ | ----------------------------------------- |
| A Neural Probabilistic Language Model                        | NNLM                                      |
| Efficient Estimation of Word Representations in Vector Space | Word2vec                                  |
| Distributed Representations of Words and Phrases and their Compositionality | Word2vec                                  |
| Neural Machine Translation by Jointly Learning to Align and Translate | Attention                                 |
| Attention Is All You Need                                    | Transformer                               |
| Deep contextualized word representations                     | ELMo                                      |
| Improving Language Understanding by Generative Pre-Training  | GPT                                       |
| BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding | BERT                                      |
| RoBERTa - A Robustly Optimized BERT Pretraining Approach     | RoBERTa                                   |
| ALBERT - A Lite BERT for Self-supervised Learning of Language Representations | ALBERT                                    |
| ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators | ELECTRA                                   |
| ERNIE - Enhanced Representation through Knowledge Integration | ERNIE(百度)                               |
| ERNIE 2.0 - A Continual Pre-training Framework for Language Understanding | ERNIE 2.0                                 |
| ERNIE-GEN - An Enhanced Multi-Flow Pre-training and Fine-tuning Framework for Natural Language Generation | ERNIE-GEN                                 |
| ERNIE - Enhanced Language Representation with Informative Entities | ERNIE(清华)                               |
| Multi-Task Deep Neural Networks for Natural Language Understanding | MT-DNN                                    |
| NEZHA - Neural Contextualized Representation for Chinese Language Understanding | NEZHA                                     |
| Pre-Training with Whole Word Masking for Chinese BERT        | Chinese-BERT-wwm                          |
| Revisiting Pre-Trained Models for Chinese Natural Language Processing | MacBERT                                   |
| SpanBERT - Improving Pre-training by Representing and Predicting Spans | SpanBERT                                  |
| Don’t Stop Pretraining - Adapt Language Models to Domains and Tasks | continue pretraining                      |
| How to Fine-Tune BERT for Text Classification?               | fine-tuning tips                          |
| Train No Evil - Selective Masking for Task-Guided Pre-Training | continue pretraining                      |
| Layer Normalization                                          | Layer Normalization                       |
| Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift | Batch Normalization                       |
| A Frustratingly Easy Approach for Joint Entity and Relation Extraction | NER & RE: Typed entity markers            |
| A Span-Extraction Dataset for Chinese Machine Reading Comprehension | CMRC 2018 dataset                         |
| A Unified MRC Framework for Named Entity Recognition         | NER: MRC method                           |
| BERT for Joint Intent Classification and Slot Filling        | Text classification & NER jointly         |
| BERT-of-Theseus - Compressing BERT by Progressive Module Replacing | Distillation: BERT-of-Theseus             |
| CLUE - A Chinese Language Understanding Evaluation Benchmark | CLUE Benchmark                            |
| CLUECorpus2020 - A Large-scale Chinese Corpus for Pre-training Language Model | CLUE corpus                               |
| Distilling Task-Specific Knowledge from BERT into Simple Neural Networks | Distillation: distill BERT into BiLSTM    |
| Distilling the Knowledge in a Neural Network                 | Distillation: Hinton                      |
| Improving Machine Reading Comprehension with Single-choice Decision and Transfer Learning | MRC: single-choice model by Tencent       |
| Language Models are Few-Shot Learners                        | GPT-3                                     |
| Language Models are Unsupervised Multitask Learners          | GPT-2                                     |
| Neural Architectures for Named Entity Recognition            | NER: BiLSTM                               |
| RACE - Large-scale ReAding Comprehension Dataset From Examinations | RACE dataset                              |
| TPLinker - Single-stage Joint Extraction of Entities and Relations Through Token Pair Linking | NER & RE: TPLinker                        |
| TextBrewer - An Open-Source Knowledge Distillation Toolkit for Natural Language Processing | Distillation: distillation toolkit by HFL |
| Two are Better than One - Joint Entity and Relation Extraction with Table-Sequence Encoders | NER & RE: Two are Better than One         |

## RecSys

| 标题                                                         | 备注       |
| ------------------------------------------------------------ | ---------- |
| Using Collaborative Filtering to Weave an Information Tapestry | CF         |
| Amazon Recommendations Item-to-Item Collaborative Filtering  | ItemCF     |
| Matrix Factorization Techniques for Recommender Systems      | MF         |
| Factorization Machines                                       | FM         |
| Field-aware Factorization Machines for CTR Prediction        | FFM        |
| Practical Lessons from Predicting Clicks on Ads at Facebook  | GBDT+LR    |
| Learning Piece-wise Linear Models from Large Scale Data for Ad Click Prediction | LS-PLM     |
| AutoRec: Autoencoders Meet Collaborative Filtering           | AutoRec    |
| Deep Neural Networks for YouTube Recommendations             | YoutubeDNN |

## Chatbot

| 标题                                                         | 备注                                       |
| ------------------------------------------------------------ | ------------------------------------------ |
| Rasa: Open Source Language Understanding and Dialogue Management | Rasa                                       |
| DIET - Lightweight Language Understanding for Dialogue Systems | DIET (Dual Intent and Entity Transformer)  |
| Dialogue Transformers                                        | TED (Transformer Embedding Dialogue)       |
| Evaluating Natural Language Understanding Services for Conversational Question Answering Systems | Evaluating NLU systems                     |
| Few-Shot Generalization Across Dialogue Tasks                | REDP (Recurrent Embedding Dialogue Policy) |
| Going Beyond T-SNE. Exposing whatlies in Text Embeddings     | whatlies                                   |
| Where is the context? A critique of recent dialogue datasets | critique of recent dialogue datasets       |

## CV

| 标题                                                         | 备注    |
| ------------------------------------------------------------ | ------- |
| ImageNet Classification with Deep Convolutional Neural Networks | AlexNet |